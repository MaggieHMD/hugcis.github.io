<!DOCTYPE html>
<html lang="en-us">
<head>


<meta charset="utf-8">
<meta name="viewport" content=
"width=device-width,initial-scale=1.0,minimum-scale=1">
<title>Neural architecture search - Hugo Cisneros - Personal
page</title>
<meta property="og:title" content=
"Neural architecture search - Hugo Cisneros - Personal page">
<meta property="og:type" content="article">
<meta property="og:image" content="/img/main.jpeg">
<meta property="og:url" content=
"https://hugocisneros.com/notes/neural_architecture_search/">
<meta property="og:description" content=
"Notes about Neural architecture search">
<meta name="Description" property="description" content=
"Notes about Neural architecture search">
<meta name="twitter:card" content="summary">
<meta name="twitter:site" content="@cisne_hug">
<meta name="twitter:creator" content="@cisne_hug">
<link rel="stylesheet" href=
"https://hugocisneros.com/css/main.min.8ea61129b6fd559b4b37a6b34ef4fc2e7dd654e21d921a711df724adeb52a29d.css"
media="all" type="text/css">
<link rel="apple-touch-icon" sizes="180x180" href=
"/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href=
"/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href=
"/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color=
"#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
<link rel="webmention" href=
"https://webmention.io/hugocisneros.com/webmention">
<link rel="pingback" href=
"https://webmention.io/hugocisneros.com/xmlrpc">
</head>
<body>
<div class="wrapper">
<header class="header">
<nav class="nav">
<div class="nav-main"><a href="https://hugocisneros.com/" class=
"nav-title">Hugo Cisneros - Personal page</a></div>
<ul class="nav-links">
<li><a href="/about/">About</a></li>
<li><a href="/blog/">Blog</a></li>
<li><a href="/other/">Other</a></li>
<li><a href="/resume/cv.pdf">Resume</a></li>
</ul>
</nav>
</header>
<main class="content" role="main">
<article class="article h-entry" itemprop="mainEntity" itemscope
itemtype="http://schema.org/BlogPosting">
<div class="single-note note-container">
<h1 class="article-title p-name" itemprop="name">Neural
architecture search</h1>
<div class="article-content e-content p-name" itemprop=
"articleBody">
<dl>
<dt>tags</dt>
<dd><a href="/notes/search/">Search</a>, <a href=
"/notes/neural_networks/">Neural networks</a></dd>
</dl>
<p>Neural architecture search (NAS) is a method for finding
<a href="/notes/neural_networks/">neural networks</a>
architectures. It is usually based on three main components:</p>
<dl>
<dt>Search space</dt>
<dd>Type of network that can be built.</dd>
<dt>Search strategy</dt>
<dd>The approach for exploring the space.</dd>
<dt>Performance estimation strategy</dt>
<dd>The way the performance of a constructed neural network is
evaluated (without actually building it or training/running
it).</dd>
</dl>
<h2 id=
"reinforcement-learning--reinforcement-learning-dot-md--based-nas">
<a href="/notes/reinforcement_learning/">Reinforcement
learning</a>-based NAS</h2>
<p>The original idea was called Neural architecture search and is
based on the use of a <a href=
"/notes/recurrent_neural_networks/">RNN</a> as a controller and
generator of architectures. The search-space is pre-defined and
explored in a rigid way. <sup id=
"a283bf303d6337f87e8d4efeb5df8b7d"><a href=
"#zophNeuralArchitectureSearch2017" title=
"Zoph \&amp; Le, Neural {{Architecture Search}} with {{Reinforcement Learning}}, {arXiv:1611.01578 [cs]}, v(), (2017).">
zophNeuralArchitectureSearch2017</a></sup>.</p>
<p>The process of generating architectures from the first article
was extremely lengthy and replaced later by a more constrained
search. <sup id="dbba5c99b9c751b32b6e58174c2800ab"><a href=
"#zophLearningTransferableArchitectures2018" title=
"Zoph, Vasudevan, Shlens \&amp; Le, Learning {{Transferable Architectures}} for {{Scalable Image Recognition}}, {arXiv:1707.07012 [cs, stat]}, v(), (2018).">
zophLearningTransferableArchitectures2018</a></sup>.</p>
<p>Recent ideas include the use of parameter sharing across
architectures because the main bottleneck of previous techniques
was essentially in the training of each child model. This results
in significant speedup of RL-based NAS. <sup id=
"fec97e39a1f34b9aa84ee30e76ef1fd0"><a href=
"#phamEfficientNeuralArchitecture2018" title=
"Pham, Guan, Zoph, Le \&amp; Dean, Efficient {{Neural Architecture Search}} via {{Parameter Sharing}}, {arXiv:1802.03268 [cs, stat]}, v(), (2018).">
phamEfficientNeuralArchitecture2018</a></sup></p>
<h2 id="neuroevolution">Neuroevolution</h2>
<p>This field is more focused on <a href=
"/notes/evolution/">evolution</a> neural network through
evolutionary methods such as e.g <a href=
"/notes/genetic_algorithm/">genetic algorithms</a>. One of the main
work that made that field popular is <a href=
"/notes/neat/">NEAT</a> <sup id=
"9af7b8b9640ea8d7a87a4d4e9ed0b9c5"><a href=
"#stanleyEvolvingNeuralNetworks2002" title=
"Stanley \&amp; Miikkulainen, Evolving {{Neural Networks}} through {{Augmenting Topologies}}, {Evolutionary Computation}, v(2), 99--127 (2002).">
stanleyEvolvingNeuralNetworks2002</a></sup>.</p>
<h1 id="bibliography">Bibliography</h1>
<p><a id=
"zophNeuralArchitectureSearch2017"></a>[zophNeuralArchitectureSearch2017]
Zoph & Le, Neural Architecture Search with Reinforcement Learning,
<i>arXiv:1611.01578 [cs]</i>, (2017). <a href=
"#a283bf303d6337f87e8d4efeb5df8b7d">↩</a></p>
<p><a id=
"zophLearningTransferableArchitectures2018"></a>[zophLearningTransferableArchitectures2018]
Zoph, Vasudevan, Shlens & Le, Learning Transferable Architectures
for Scalable Image Recognition, <i>arXiv:1707.07012 [cs, stat]</i>,
(2018). <a href="#dbba5c99b9c751b32b6e58174c2800ab">↩</a></p>
<p><a id=
"phamEfficientNeuralArchitecture2018"></a>[phamEfficientNeuralArchitecture2018]
Pham, Guan, Zoph, Le & Dean, Efficient Neural Architecture Search
via Parameter Sharing, <i>arXiv:1802.03268 [cs, stat]</i>, (2018).
<a href="#fec97e39a1f34b9aa84ee30e76ef1fd0">↩</a></p>
<p><a id=
"stanleyEvolvingNeuralNetworks2002"></a>[stanleyEvolvingNeuralNetworks2002]
Stanley & Miikkulainen, Evolving Neural Networks through Augmenting
Topologies, <i>Evolutionary Computation</i>, <b>10(2)</b>, 99-127
(2002). <a href=
"http://dx.doi.org/10.1162/106365602320169811">doi</a>. <a href=
"#9af7b8b9640ea8d7a87a4d4e9ed0b9c5">↩</a></p>
<h2 id="backlinks">Backlinks</h2>
<ul>
<li><a href="/notes/stanleyevolvingneuralnetworks2002/">Notes on:
Evolving Neural Networks through Augmenting Topologies by Stanley,
K. O., & Miikkulainen, R. (2002)</a></li>
<li><a href=
"/notes/floreanoneuroevolutionarchitectureslearning2008/">Notes on:
Neuroevolution: from architectures to learning by Floreano, D.,
Dürr, P., & Mattiussi, C. (2008)</a></li>
<li><a href=
"/notes/zophlearningtransferablearchitectures2018/">Notes on:
Learning Transferable Architectures for Scalable Image Recognition
by Zoph, B., Vasudevan, V., Shlens, J., & Le, Q. V. (2018)</a></li>
<li><a href="/notes/zophneuralarchitecturesearch2017/">Notes on:
Neural Architecture Search with Reinforcement Learning by Zoph, B.,
& Le, Q. V. (2017)</a></li>
</ul>
</div>
<div class="note-footer">Last changed <a class="u-url" href=
"https://hugocisneros.com/notes/neural_architecture_search/"><time itemprop="datePublished"
class="dt-published" datetime=
"2020-07-27T14:13:00+0200">27/07/2020</time></a> | authored by
<a href="https://hugocisneros.com/" rel="author" class=
"p-author h-card" itemprop="author" itemscope itemtype=
"http://schema.org/Person"><span itemprop="name">Hugo
Cisneros</span></a></div>
</div>
</article>
<br>
<a href="/notes#neural_architecture_search"><b>← Back to
Notes</b></a>
<hr></main>
<footer class="footer">
<ul class="footer-links">
<li><a class="rss-link" href="/blog/index.xml" type=
"application/rss+xml" target="_blank">Blog <img class="rss-icon"
src="/img/RSS.svg" alt="RSS feed icon"></a></li>
<li><a href=
"https://github.com/hugcis/natrium-custom">Code</a></li>
<li>© Hugo Cisneros 2020</li>
</ul>
</footer>
</div>
<script>
 MathJax = {
     tex: {
         inlineMath: [['$','$'], ['\\(', '\\)']],
         tags: 'ams'
     }
 };
</script> 
<script type="text/javascript" rel="preconnect" id="MathJax-script"
async src=
"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
