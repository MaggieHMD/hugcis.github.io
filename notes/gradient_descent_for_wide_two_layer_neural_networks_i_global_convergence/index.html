<!DOCTYPE html>
<html lang="en-us">
<head>


<meta charset="utf-8">
<meta name="viewport" content=
"width=device-width,initial-scale=1.0,minimum-scale=1">
<title>Gradient descent for wide two-layer neural networks – I :
Global convergence - Hugo Cisneros - Personal page</title>
<meta property="og:title" content=
"Gradient descent for wide two-layer neural networks – I : Global convergence - Hugo Cisneros - Personal page">
<meta property="og:type" content="article">
<meta property="og:image" content="/img/main.jpeg">
<meta property="og:url" content=
"https://hugocisneros.com/notes/gradient_descent_for_wide_two_layer_neural_networks_i_global_convergence/">
<meta property="og:description" content=
"Notes about Gradient descent for wide two-layer neural networks – I : Global convergence">
<meta name="Description" property="description" content=
"Notes about Gradient descent for wide two-layer neural networks – I : Global convergence">
<meta name="twitter:card" content="summary">
<meta name="twitter:site" content="@cisne_hug">
<meta name="twitter:creator" content="@cisne_hug">
<link rel="stylesheet" href=
"https://hugocisneros.com/css/main.min.49a178b6a182a13a256ca7e09069d804e8578b4b3ff071f060fa3ea637de481e.css"
media="all" type="text/css">
<link rel="apple-touch-icon" sizes="180x180" href=
"/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href=
"/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href=
"/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color=
"#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
<link rel="webmention" href=
"https://webmention.io/hugocisneros.com/webmention">
<link rel="pingback" href=
"https://webmention.io/hugocisneros.com/xmlrpc">
</head>
<body>
<div class="wrapper">
<header class="header">
<nav class="nav">
<div class="nav-main"><a href="https://hugocisneros.com/" class=
"nav-title">Hugo Cisneros - Personal page</a></div>
<ul class="nav-links">
<li><a href="/about/">About</a></li>
<li><a href="/blog/">Blog</a></li>
<li><a href="/other/">Other</a></li>
<li><a href="/resume/cv.pdf">Resume</a></li>
</ul>
</nav>
</header>
<main class="content" role="main">
<article class="article h-entry" itemprop="mainEntity" itemscope
itemtype="http://schema.org/BlogPosting">
<div class="single-note note-container">
<h1 class="article-title p-name" itemprop="name">Gradient descent
for wide two-layer neural networks – I : Global convergence</h1>
<div class="article-content e-content p-name" itemprop=
"articleBody">
<dl>
<dt>tags</dt>
<dd><a href="/notes/neural_networks/">Neural networks</a>, <a href=
"/notes/optimization/">Optimization</a></dd>
<dt>authors</dt>
<dd>Francis Bach, Lénaïc Chizat</dd>
<dt>source</dt>
<dd><a href=
"https://francisbach.com/gradient-descent-neural-networks-global-convergence/">
Francis Bach’s blog</a></dd>
</dl>
<p>In the rest, we use the mathematical definition of a neural
network shown in <a href="/notes/neural_networks/">Neural
networks</a>.</p>
<h2 id="two-layer-neural-network">Two layer neural network</h2>
<p>Even simple neural network models are very difficult to analyze.
This is primarily due to two difficulties:</p>
<ul>
<li><strong>Non-linearity</strong>: the problem is typically
non-convex, which in general is a bad thing in optimization.</li>
<li><strong>Overparametrization</strong>: there are often a lot of
parameters, sometimes many more parameters than observations.</li>
</ul>
<p>Results presented here are actually taking advantage of
overparametrization, with \(m\rightarrow \infty\) and two key
properties of the problem.</p>
<ul>
<li><strong>Separability</strong>: The problem can be decomposed in
a sum of terms independently parametrized in \(\omega_i = (a_i,
b_i)\), with \(h = \frac{1}{m} \sum_{i=1}^m \Phi(\omega_i)\) where
\(\Phi : \mathbb{R}^p \rightarrow \mathcal{F}\) and $\mathcal{F} is
a space of functions. Here, \(p = d+1\) and \[ \Phi(w)(x) = a
(b^\top x)_+. \] This part is only true for two-layer neural
networks however.</li>
<li><strong>Homogeneity</strong>: relu is positively homogeneous
and \(\Phi\) is 2-homogeneous, meaning that for \(\omega = (a,
b)\), \(\Phi(\lambda\omega) = \lambda^2 \Phi(\omega)\).</li>
</ul>
</div>
<div class="note-footer">Last changed <a class="u-url" href=
"https://hugocisneros.com/notes/gradient_descent_for_wide_two_layer_neural_networks_i_global_convergence/">
<time itemprop="datePublished" class="dt-published" datetime=
"2020-06-23T09:16:58+0200">23/06/2020</time></a> | authored by
<a href="https://hugocisneros.com/" rel="author" class=
"p-author h-card" itemprop="author" itemscope itemtype=
"http://schema.org/Person"><span itemprop="name">Hugo
Cisneros</span></a></div>
</div>
</article>
<br>
<a href=
"/notes#gradient_descent_for_wide_two_layer_neural_networks_i_global_convergence">
<b>← Back to Notes</b></a>
<hr></main>
<footer class="footer">
<ul class="footer-links">
<li><a href="/blog/index.xml" type="application/rss+xml" target=
"_blank">Blog RSS feed</a></li>
<li><a href=
"https://github.com/hugcis/natrium-custom">Code</a></li>
</ul>
</footer>
</div>
<script>
 MathJax = {
     tex: {
         inlineMath: [['$','$'], ['\\(', '\\)']],
         tags: 'ams'
     }
 };
</script> 
<script type="text/javascript" rel="preconnect" id="MathJax-script"
async src=
"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
