<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Notes on Hugo Cisneros - Personal page</title>
    <link>https://hugocisneros.com/notes/</link>
    <description>Recent content in Notes on Hugo Cisneros - Personal page</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 20 Jun 2020 19:06:49 +0200</lastBuildDate>
    
	<atom:link href="https://hugocisneros.com/notes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Abelian sandpile model</title>
      <link>https://hugocisneros.com/notes/abelian_sandpile_model/</link>
      <pubDate>Wed, 01 Jul 2020 20:43:05 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/abelian_sandpile_model/</guid>
      <description> tags Cellular automata  </description>
    </item>
    
    <item>
      <title>Adversarial examples</title>
      <link>https://hugocisneros.com/notes/adversarial_examples/</link>
      <pubDate>Wed, 01 Jul 2020 20:43:05 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/adversarial_examples/</guid>
      <description> tags Machine learning, Neural networks  Adversarial examples in Reinforcement learning Backlinks  Talk: Artificial Intelligence: A Guide for Thinking Humans  </description>
    </item>
    
    <item>
      <title>Algorithmic Information theory</title>
      <link>https://hugocisneros.com/notes/algorithmic_information_theory/</link>
      <pubDate>Wed, 01 Jul 2020 20:43:05 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/algorithmic_information_theory/</guid>
      <description>Backlinks  Minimum description length Kolmogorov complexity Algorithmic probability Complexity metrics Halting probability  </description>
    </item>
    
    <item>
      <title>Algorithmic probability</title>
      <link>https://hugocisneros.com/notes/algorithmic_probability/</link>
      <pubDate>Wed, 01 Jul 2020 20:43:05 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/algorithmic_probability/</guid>
      <description> tags Complexity, Algorithmic Information theory  </description>
    </item>
    
    <item>
      <title>Applied maths</title>
      <link>https://hugocisneros.com/notes/applied_maths/</link>
      <pubDate>Wed, 01 Jul 2020 20:43:05 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/applied_maths/</guid>
      <description> tags Mathematics  Backlinks  SIR model Statistics Optimal transport Image processing Cryptography  </description>
    </item>
    
    <item>
      <title>Artificial Intelligence</title>
      <link>https://hugocisneros.com/notes/artificial_intelligence/</link>
      <pubDate>Wed, 01 Jul 2020 20:43:05 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/artificial_intelligence/</guid>
      <description>Backlinks  Notes on: Anderson, P. W. (1972): More Is Different NLP Open-ended Evolution Machine learning Compression Notes on: Clune, J. (2019): AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence Talk: Artificial Intelligence: A Guide for Thinking Humans Why programming is a good medium for expressing poorly understood and sloppily-formulated ideas Open-endedness: The last grand challenge youâ€™ve never heard of Cellular automata Alternative learning mechanisms  </description>
    </item>
    
    <item>
      <title>Artificial life</title>
      <link>https://hugocisneros.com/notes/artificial_life/</link>
      <pubDate>Wed, 01 Jul 2020 20:43:05 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/artificial_life/</guid>
      <description>Backlinks  Notes on: Anderson, P. W. (1972): More Is Different Open-ended Evolution Notes on: Kruszewski, G., &amp;amp; Mikolov, T. (2020): Combinatory Chemistry: Towards a Simple Model of Emergent Evolution Evolution  </description>
    </item>
    
    <item>
      <title>Autoencoders</title>
      <link>https://hugocisneros.com/notes/autoencoders/</link>
      <pubDate>Wed, 01 Jul 2020 20:43:05 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/autoencoders/</guid>
      <description>tags Neural networks, Data representation  Autoencoders and PCA Added: [2020-02-25 Tue 21:21]
The relation between Autoencoders and PCA is strong. In particular, a very small autoencoder with only linear activations seems intuitively very close to PCA decomposition. (Bourlard &amp;amp; Kamp, 1988) gives an interesting analysis of the uselessness of the activation functions in the encoding layers of an autoencoder when there is no activations in the output layers. In that case, autoencoding is closely related to a sinigular value decomposition of the input data.</description>
    </item>
    
    <item>
      <title>Automated discovery in complex systems</title>
      <link>https://hugocisneros.com/notes/automated_discovery_in_complex_systems/</link>
      <pubDate>Wed, 01 Jul 2020 20:43:05 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/automated_discovery_in_complex_systems/</guid>
      <description>tags Complex Systems  Evolutionary algorithms and CAs Evolutionary algorithms have been used to find Cellular automata rules with specific behavior (Mitchell et al., 1996), (Sapin et al., 2003) . The objective is to optimize a fitness function (majority of cells, presence of gliders and periodic patterns, etc.).
Bibliography Mitchell, M., Road, H. P., Das, R., &amp;amp; Box, P. O., Evolving Cellular Automata with Genetic Algorithms: A Review of Recent Work, In , Proceedings of the {{First International Conference}} on {{Evolutionary Computation}} and {{Its Applications} (pp.</description>
    </item>
    
    <item>
      <title>Backward RNN</title>
      <link>https://hugocisneros.com/notes/backward_rnn/</link>
      <pubDate>Wed, 01 Jul 2020 20:43:05 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/backward_rnn/</guid>
      <description>tags Recurrent neural networks  Regular RNNs process input in sequence. When applied to a language modeling task, one tries to predict a word given the previous ones. For example, with the sentence The quick brown fox jumps over the lazy, a classical RNN will initialize and internal state \(s_0\) and process each word in sequence, starting from The and updating its internal state with each new word in order to make a final prediction.</description>
    </item>
    
  </channel>
</rss>