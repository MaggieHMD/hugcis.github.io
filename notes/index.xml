<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Notes on Hugo Cisneros - Personal page</title>
    <link>https://hugocisneros.com/notes/</link>
    <description>Recent content in Notes on Hugo Cisneros - Personal page</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="https://hugocisneros.com/notes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Abelian sandpile model</title>
      <link>https://hugocisneros.com/notes/20200507101434-abelian_sandpile_model/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hugocisneros.com/notes/20200507101434-abelian_sandpile_model/</guid>
      <description> tags Cellular automata  </description>
    </item>
    
    <item>
      <title>Adversarial examples</title>
      <link>https://hugocisneros.com/notes/20200330105209-adversarial_examples/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hugocisneros.com/notes/20200330105209-adversarial_examples/</guid>
      <description> tags Machine learning, Neural networks  Adversarial examples in Reinforcement learning </description>
    </item>
    
    <item>
      <title>Algorithmic Information theory</title>
      <link>https://hugocisneros.com/notes/20200302174626_algorithmic_information_theory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hugocisneros.com/notes/20200302174626_algorithmic_information_theory/</guid>
      <description>Backlinks  Complexity Kolmogorov complexity Algorithmic probability Halting probability Minimum description length  </description>
    </item>
    
    <item>
      <title>Algorithmic probability</title>
      <link>https://hugocisneros.com/notes/20200307161827-algorithmic_probability/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hugocisneros.com/notes/20200307161827-algorithmic_probability/</guid>
      <description> tags complexity, algorithmic information theory  </description>
    </item>
    
    <item>
      <title>Artificial Intelligence</title>
      <link>https://hugocisneros.com/notes/20200301201821_artificial_intelligence/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hugocisneros.com/notes/20200301201821_artificial_intelligence/</guid>
      <description>Backlinks  Notes on: Clune, J. (2019): AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence Open-ended Evolution Machine learning Compression Open-endedness: The last grand challenge youâ€™ve never heard of Notes on: Anderson, P. W. (1972): More Is Different Cellular automata  </description>
    </item>
    
    <item>
      <title>Artificial life</title>
      <link>https://hugocisneros.com/notes/20200301203351_artificial_life/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hugocisneros.com/notes/20200301203351_artificial_life/</guid>
      <description>Backlinks  Notes on: Kruszewski, G., &amp;amp; Mikolov, T. (2020): Combinatory Chemistry: Towards a Simple Model of Emergent Evolution Open-ended Evolution Evolution Notes on: Anderson, P. W. (1972): More Is Different  </description>
    </item>
    
    <item>
      <title>Autoencoders</title>
      <link>https://hugocisneros.com/notes/20200301200248_autoencoders/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hugocisneros.com/notes/20200301200248_autoencoders/</guid>
      <description>tags Neural networks, Data representation  Autoencoders and PCA Added: [2020-02-25 Tue 21:21]
The relation between Autoencoders and PCA is strong. In particular, a very small autoencoder with only linear activations seems intuitively very close to PCA decomposition. (Bourlard &amp;amp; Kamp, 1988) gives an interesting analysis of the uselessness of the activation functions in the encoding layers of an autoencoder when there is no activations in the output layers. In that case, autoencoding is closely related to a sinigular value decomposition of the input data.</description>
    </item>
    
    <item>
      <title>Backward RNN</title>
      <link>https://hugocisneros.com/notes/20200506180851-backward_rnn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hugocisneros.com/notes/20200506180851-backward_rnn/</guid>
      <description>tags Recurrent neural networks  Regular RNNs process input in sequence. When applied to a language modeling task, one tries to predict a word given the previous ones. For example, with the sentence The quick brown fox jumps over the lazy, a classical RNN will initialize and internal state \(s_0\) and process each word in sequence, starting from The and updating its internal state with each new word in order to make a final prediction.</description>
    </item>
    
    <item>
      <title>Berry&#39;s paradox</title>
      <link>https://hugocisneros.com/notes/20200306223742-berry_s_paradox/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hugocisneros.com/notes/20200306223742-berry_s_paradox/</guid>
      <description>Berry&amp;rsquo;s paradox is a sentence of the form &amp;ldquo;The smallest positive integer not definable in under sixty letters&amp;rdquo; (a phrase with fifty-seven letters).
An argument very similar to Berry&amp;rsquo;s paradox is used in the proof of uncomputability of Kolmogorov complexity.
Resolution An interesting study and resolution of Berry&amp;rsquo;s paradox</description>
    </item>
    
    <item>
      <title>Boltzmann brain</title>
      <link>https://hugocisneros.com/notes/20200301202542_boltzmann_brain/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://hugocisneros.com/notes/20200301202542_boltzmann_brain/</guid>
      <description>tags Physics, Statistical physics  Boltzmann brain Boltzmann brain is an interesting concept offered initially in response to one of Ludwig Boltzmann&amp;rsquo;s explanation for the low-entropy state of the Universe. He hypothesized that even a fully random universe would fluctuate towards lower-entropy states. The issue is that many phenomena such as evolved life on Earth are so far from equilibrium it looks like they were extremely unlikely to have happened.</description>
    </item>
    
  </channel>
</rss>