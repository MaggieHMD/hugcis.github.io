<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Notes on Hugo Cisneros - Personal page</title>
    <link>https://hugocisneros.com/notes/</link>
    <description>Recent content in Notes on Hugo Cisneros - Personal page</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 20 Jun 2020 19:06:49 +0200</lastBuildDate>
    
	<atom:link href="https://hugocisneros.com/notes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Turing completeness of cellular automata</title>
      <link>https://hugocisneros.com/notes/turing_completeness_of_cellular_automata/</link>
      <pubDate>Wed, 01 Jul 2020 14:25:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/turing_completeness_of_cellular_automata/</guid>
      <description> tags Cellular automata, Turing-completeness  Rule 110 Elementary cellular automaton rule 110 is universal (Cook, 2004).
Game of Life Conway&amp;rsquo;s Game of Life has also been show to be Turing-complete. Gliders can be used to implement logic gates.
Bibliography Cook, M., Universality in Elementary Cellular Automata, Complex Systems, (), 40 (2004).  ↩
Backlinks  Turing-completeness  </description>
    </item>
    
    <item>
      <title>Notes on: Katharopoulos, A., Vyas, A., Pappas, N., &amp; Fleuret, F. (2020): Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention</title>
      <link>https://hugocisneros.com/notes/katharopoulostransformersarernns2020/</link>
      <pubDate>Wed, 01 Jul 2020 09:31:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/katharopoulostransformersarernns2020/</guid>
      <description>tags Transformers, RNN source (Katharopoulos et al., 2020)  Summary Transformers have traditionally been described as different models from RNNs. This is because instead of processing the sequence one token at a time, Transformers use attention to process all elements simultaneously.
The paper introduces an interesting new formulation, replacing the softmax attention with a feature map-based dot product.
This new formulation yields better time and memory complexity as well as a model that is casual and autoregressive (similar to RNNs).</description>
    </item>
    
    <item>
      <title>Kernel Methods</title>
      <link>https://hugocisneros.com/notes/kernel_methods/</link>
      <pubDate>Wed, 01 Jul 2020 08:14:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/kernel_methods/</guid>
      <description> tags Machine learning  Backlinks  Notes on: Katharopoulos, A., Vyas, A., Pappas, N., &amp;amp; Fleuret, F. (2020): Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention  </description>
    </item>
    
    <item>
      <title>Attention</title>
      <link>https://hugocisneros.com/notes/attention/</link>
      <pubDate>Tue, 30 Jun 2020 15:16:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/attention/</guid>
      <description>tags Neural networks  Self-attention is a weighted average of all input elements from a sequence, with a weight proportional to a similarity score between representations. The input \(x \in \mathbb{R}^{L \times F}\) is projected by matrices \(W_Q \in \mathbb{R}^{F \times D}\), \(W_K \in \mathbb{R}^{F\times D}\) and \(W_V \in \mathbb{R}^{F\times M}\) to representations \(Q\) (queries), \(K\) (keys) and \(V\) (values).
\[ Q = xW_Q\] \[ K = xW_K\] \[ V = xW_V\]</description>
    </item>
    
    <item>
      <title>C&#43;&#43;</title>
      <link>https://hugocisneros.com/notes/c/</link>
      <pubDate>Tue, 30 Jun 2020 13:30:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/c/</guid>
      <description> tags Programming languages, Coding  Backlinks  Programming languages  </description>
    </item>
    
    <item>
      <title>NEAT</title>
      <link>https://hugocisneros.com/notes/neat/</link>
      <pubDate>Tue, 30 Jun 2020 13:12:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/neat/</guid>
      <description>Backlinks  Neural architecture search  </description>
    </item>
    
    <item>
      <title>Notes on: Horowitz, J. M., &amp; England, J. L. (2017): Spontaneous fine-tuning to environment in many-species chemical reaction networks</title>
      <link>https://hugocisneros.com/notes/horowitzspontaneousfinetuningenvironment2017/</link>
      <pubDate>Mon, 29 Jun 2020 18:39:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/horowitzspontaneousfinetuningenvironment2017/</guid>
      <description>source (Horowitz &amp;amp; England, 2017)  TODO Summary TODO Comments Bibliography Horowitz, J. M., &amp;amp; England, J. L., Spontaneous fine-tuning to environment in many-species chemical reaction networks, Proceedings of the National Academy of Sciences, 114(29), 7565–7570 (2017). http://dx.doi.org/10.1073/pnas.1700617114 ↩</description>
    </item>
    
    <item>
      <title>Chemical reaction network</title>
      <link>https://hugocisneros.com/notes/chemical_reaction_network/</link>
      <pubDate>Mon, 29 Jun 2020 17:46:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/chemical_reaction_network/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Implicit neural representations</title>
      <link>https://hugocisneros.com/notes/implicit_neural_representations/</link>
      <pubDate>Mon, 29 Jun 2020 16:35:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/implicit_neural_representations/</guid>
      <description>tags Data representation, Neural networks  Implicit neural representations is about parameterizing a continuous differentiable signal with a neural network. The signal is encoded within the neural network, providing a possibly more compact representation.
Applications of these learned representations range from simple compression, to 3D scene reconstruction from 2D images, semantic information inference, etc.
CPPN is an early example of a implicit neural representation implementation mainly used for pattern generation .</description>
    </item>
    
    <item>
      <title>Alternative learning mechanisms</title>
      <link>https://hugocisneros.com/notes/alternative_learning_mechanisms/</link>
      <pubDate>Sun, 28 Jun 2020 19:21:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/alternative_learning_mechanisms/</guid>
      <description>tags Machine learning Many people, including Geoffrey Hinton, have raised concerns about the back-propagation algorithm and the fact that it&amp;rsquo;s likely not a promising way to achieve Artificial Intelligence (see this Axios blog post).
  Alternative mechanisms for learning have been and are currently studied to try and approach the learning problem in a more effective way.
Direct feedback alignment (N\okland, 2016)
Bibliography N\okland, Arild, Direct Feedback Alignment Provides Learning in Deep Neural Networks, arXiv:1609.</description>
    </item>
    
  </channel>
</rss>