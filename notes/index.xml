<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Notes on Hugo Cisneros - Personal page</title>
    <link>https://hugocisneros.com/notes/</link>
    <description>Recent content in Notes on Hugo Cisneros - Personal page</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 13 Jul 2020 18:45:00 +0200</lastBuildDate>
    
	<atom:link href="https://hugocisneros.com/notes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Notes on: Efficient Neural Architecture Search via Parameter Sharing by Pham, H., Guan, M. Y., Zoph, B., Le, Q. V., &amp; Dean, J. (2018)</title>
      <link>https://hugocisneros.com/notes/phamefficientneuralarchitecture2018/</link>
      <pubDate>Mon, 13 Jul 2020 18:45:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/phamefficientneuralarchitecture2018/</guid>
      <description>source (Pham et al., 2018)  Summary Like other papers, the controller is a RNN that generates each part of the architecture in sequence. The main contribution of this paper is to introduce parameter sharing in child models. For, this, it represents all possible architectures in a single DAG of operations and share weights between same operations. They explain how to design a RNN cell with their model, a convolutional network (and convolutional cell to build a CNN) and how to train.</description>
    </item>
    
    <item>
      <title>Notes on: Intrinsically Motivated Discovery of Diverse Patterns in Self-Organizing Systems by Reinke, C., Etcheverry, M., &amp; Oudeyer, P. (2020)</title>
      <link>https://hugocisneros.com/notes/reinkeintrinsicallymotivateddiscovery2020/</link>
      <pubDate>Mon, 13 Jul 2020 18:45:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/reinkeintrinsicallymotivateddiscovery2020/</guid>
      <description>source (Reinke et al., 2020)  Summary The authors address the problem of automated discovery of diverse self-organized patterns in high-dimensional and complex game-of-life types of dynamical systems. They conduct experiments on Lenia.
Their goal is to use an IMGEP algorithm to represent interesting patterns and discover them.
Problem setting Goal: With a budget of \(N\) experiments, maximize diversity of observations.
Parameter space \(\Theta\) of available parameters \(\theta\). An observation space \(O\) of observations.</description>
    </item>
    
    <item>
      <title>Notes on: Seeking open-ended evolution in Swarm Chemistry by Sayama, H. (2011)</title>
      <link>https://hugocisneros.com/notes/sayamaseekingopenendedevolution2011/</link>
      <pubDate>Mon, 13 Jul 2020 18:45:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/sayamaseekingopenendedevolution2011/</guid>
      <description>tags Open-ended Evolution source (Sayama, 2011)  TODO Summary TODO Comments Bibliography Sayama, H., Seeking open-ended evolution in Swarm Chemistry, In , 2011 {{IEEE Symposium}} on {{Artificial Life}} ({{ALIFE}}) (pp. 186–193) (2011). Paris, France: IEEE. ↩</description>
    </item>
    
    <item>
      <title>Notes on: A Computer Scientist&#39;s View of Life, the Universe, and Everything by Schmidhuber, J. (1999)</title>
      <link>https://hugocisneros.com/notes/schmidhubercomputerscientistview1999/</link>
      <pubDate>Mon, 13 Jul 2020 18:44:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/schmidhubercomputerscientistview1999/</guid>
      <description>tags Zuse&amp;rsquo;s thesis source (Schmidhuber, 1999)  Summary Comments Bibliography Schmidhuber, J., A Computer Scientist&amp;rsquo;s View of Life, the Universe, and Everything, arXiv:quant-ph/9904050, (), (1999).  ↩</description>
    </item>
    
    <item>
      <title>Notes on: Evolving Neural Networks through Augmenting Topologies by Stanley, K. O., &amp; Miikkulainen, R. (2002)</title>
      <link>https://hugocisneros.com/notes/stanleyevolvingneuralnetworks2002/</link>
      <pubDate>Mon, 13 Jul 2020 18:44:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/stanleyevolvingneuralnetworks2002/</guid>
      <description>tags Neural networks, Genetic algorithms, NAS source (Stanley &amp;amp; Miikkulainen, 2002)  Summary This is the main paper introducing the NEAT system. This system is a direct-encoding based way of dealing with neuroevolution (evolution of ANNs). The encoding is based on a genome sequentially specifying each of the connections between modules of the network. Several tickes are used to make it possible applying GA methods to evolve networks:
 Historical tracking of genes to be able to align architectures and mate them.</description>
    </item>
    
    <item>
      <title>Notes on: The Architecture of Complexity by Simon, H. A. (1962)</title>
      <link>https://hugocisneros.com/notes/simonarchitecturecomplexity1962/</link>
      <pubDate>Mon, 13 Jul 2020 18:44:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/simonarchitecturecomplexity1962/</guid>
      <description>tags Complexity, Complex Systems source (Simon, 1962)  Complex systems  In such systems, the whole is more than the sum of the parts, not in an ultimate, metaphysical sense, but in the important pragmatic sense that, given the properties of the parts and the laws of their interaction, it is not a trivial matter to infer the properties of the whole. In the face of complexity, an in-principle reductionist may be at the same time a pragmatic holist.</description>
    </item>
    
    <item>
      <title>Notes on: Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks by Voelker, A., Kajić, I., &amp; Eliasmith, C. (2019)</title>
      <link>https://hugocisneros.com/notes/voelkerlegendrememoryunits2019/</link>
      <pubDate>Mon, 13 Jul 2020 18:42:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/voelkerlegendrememoryunits2019/</guid>
      <description>tags Recurrent neural networks source (Voelker et al., 2019)  Summary This paper introduces the LMU recurrent cell. This cell is based on a similar-ish idea from LSTM to maintain a memory hidden state. The main idea of the paper is to make this memory satisfy a set of first order ordinary differential equations. \begin{equation} θ \dot{m}(t) = Am(t) + Bu(t) \end{equation} This system has a solution which represents sliding windows of \(u\) via Legendre polynomials.</description>
    </item>
    
    <item>
      <title>Notes on: Modeling systems with internal state using evolino by Wierstra, D., Gomez, F. J., &amp; Schmidhuber, J. (2005)</title>
      <link>https://hugocisneros.com/notes/wierstramodelingsystemsinternal2005/</link>
      <pubDate>Mon, 13 Jul 2020 18:41:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/wierstramodelingsystemsinternal2005/</guid>
      <description>tags Genetic algorithms, Recurrent neural networks source (Wierstra et al., 2005)  TODO Summary TODO Comments Bibliography Wierstra, D., Gomez, F. J., &amp;amp; Schmidhuber, J&amp;quot;urgen, Modeling systems with internal state using evolino, In , Proceedings of the 2005 Conference on {{Genetic}} and Evolutionary Computation - {{GECCO}} &amp;lsquo;05 (pp. 1795) (2005). {Washington DC, USA}: {ACM Press}. ↩</description>
    </item>
    
    <item>
      <title>Notes on: POET: open-ended coevolution of environments and their optimized solutions by Wang, R., Lehman, J., Clune, J., &amp; Stanley, K. O. (2019)</title>
      <link>https://hugocisneros.com/notes/wangpoetopenendedcoevolution2019/</link>
      <pubDate>Mon, 13 Jul 2020 18:41:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/wangpoetopenendedcoevolution2019/</guid>
      <description>tags Open-ended Evolution, Reinforcement learning source (Wang et al., 2019)  Summary This paper is about introducing the POET architecture. The core idea behind this framework is to build a system that can make agents learn complex behavior through joint evolution of agents and the environment. The better the agent, the more complex environment we can give it.
There are 3 main components to the algorithm: an evolutionary strategy (ES) for the environment itself, resembling genetic algorithm, another ES for the agents (although these agents might also be trained with RL), and a transfer mechanism whereby agents trained in a particular environment can be trained on another one.</description>
    </item>
    
    <item>
      <title>Online privacy</title>
      <link>https://hugocisneros.com/notes/online_privacy/</link>
      <pubDate>Mon, 13 Jul 2020 18:40:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/online_privacy/</guid>
      <description>IP Addresses In (Mishra et al., 2020), the authors analyze a set of users&amp;rsquo; internet traffic for more than 100 days. They observed a little more than 11% of the 34,488 IP addresses they collected were present for more than a month. Many of them were reused throughout the whole experience, making long-term tracking of users possible.
The study also shows that 93% of users had a unique fixed set of IP addresses during the whole experiment, making it easy to track them between home, work, etc.</description>
    </item>
    
  </channel>
</rss>